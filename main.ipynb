{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919dee5e-fade-4973-b57e-be0a3bd2c60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from opts import parse_opts\n",
    "from model import generate_model\n",
    "import transforms \n",
    "from dataset import get_training_set, get_validation_set, get_test_set\n",
    "from utils import Logger, adjust_learning_rate, save_checkpoint\n",
    "from train import train_epoch\n",
    "from validation import val_epoch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57feff34-924c-4603-bc7a-7c143efbb80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/majidh/anaconda3/envs/mml/lib/python3.8/site-packages/ipykernel_launcher.py',\n",
       " '-f',\n",
       " '/home/majidh/.local/share/jupyter/runtime/kernel-ce4a7848-ee56-48b1-8d2b-2d73af163e73.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1169e2d6-438f-4132-944d-7f0637e04bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(annotation_path='ravdess_preprocessing/annotations.csv', batch_size=8, begin_epoch=1, dampening=0.9, dataset='RAVDESS', device='cuda', fusion='ia', learning_rate=0.04, lr_patience=10, lr_steps=[40, 55, 65, 70, 200, 250], manual_seed=1, mask='softhard', model='multimodalcnn', momentum=0.9, n_classes=8, n_epochs=100, n_threads=16, no_train=False, no_val=False, num_heads=1, pretrain_path='EfficientFace_Trained_on_AffectNet7.pth.tar', result_path='results', resume_path='', sample_duration=15, sample_size=224, store_name='model', test=True, test_subset='test', video_norm_value=255, weight_decay=0.001)\n"
     ]
    }
   ],
   "source": [
    "model = ''\n",
    "dataset = ''\n",
    "sample_duration = ''\n",
    "\n",
    "sys.argv = ['main.py']\n",
    "\n",
    "# Parsing command line option\n",
    "opt = parse_opts()\n",
    "print(opt)\n",
    "\n",
    "n_folds = 1\n",
    "test_accuracies = []\n",
    "\n",
    "#Opt between CPU or GPU \n",
    "if opt.device != 'cpu':\n",
    "    opt.device = 'cuda' if torch.cuda.is_available() else 'cpu'  \n",
    "\n",
    "# specifies that the model is pretrained or not\n",
    "pretrained = opt.pretrain_path != 'None'    \n",
    "\n",
    "#make results folder \n",
    "if not os.path.exists(opt.result_path):\n",
    "    os.makedirs(opt.result_path)\n",
    "\n",
    "opt.arch = '{}'.format(opt.model)  \n",
    "opt.store_name = '_'.join([opt.dataset, opt.model, str(opt.sample_duration)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc09915a-dbe2-480a-a05b-51c364898e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(annotation_path='ravdess_preprocessing/annotations.csv', arch='multimodalcnn', batch_size=8, begin_epoch=1, dampening=0.9, dataset='RAVDESS', device='cuda', fusion='ia', learning_rate=0.04, lr_patience=10, lr_steps=[40, 55, 65, 70, 200, 250], manual_seed=1, mask='softhard', model='multimodalcnn', momentum=0.9, n_classes=8, n_epochs=100, n_threads=16, no_train=False, no_val=False, num_heads=1, pretrain_path='EfficientFace_Trained_on_AffectNet7.pth.tar', result_path='results', resume_path='', sample_duration=15, sample_size=224, store_name='RAVDESS_multimodalcnn_15', test=True, test_subset='test', video_norm_value=255, weight_decay=0.001)\n",
      "Initializing efficientnet\n",
      "Total number of trainable parameters:  1854766\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m criterion \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mto(opt\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt\u001b[38;5;241m.\u001b[39mno_train:\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#define training data\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     training_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_training_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# we change the spatial transform to none to prevent generating random noise  \u001b[39;00m\n\u001b[1;32m     20\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     21\u001b[0m         training_data,\n\u001b[1;32m     22\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m     23\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mn_threads,\n\u001b[1;32m     25\u001b[0m         pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m     train_logger \u001b[38;5;241m=\u001b[39m Logger(\n\u001b[1;32m     28\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(opt\u001b[38;5;241m.\u001b[39mresult_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(fold)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.log\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     29\u001b[0m         [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprec1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprec5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/nfsmount/majid/multimodal/Emotion_recognition_T/dataset.py:7\u001b[0m, in \u001b[0;36mget_training_set\u001b[0;34m(opt, spatial_transform, audio_transform)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m opt\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRAVDESS\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported dataset: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(opt\u001b[38;5;241m.\u001b[39mdataset))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRAVDESS\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 7\u001b[0m     training_data \u001b[38;5;241m=\u001b[39m \u001b[43mRAVDESS\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotation_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspatial_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBiovisual\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m training_data\n",
      "File \u001b[0;32m/nfsmount/majid/multimodal/Emotion_recognition_T/datasets/ravdess.py:61\u001b[0m, in \u001b[0;36mRAVDESS.__init__\u001b[0;34m(self, annotation_path, subset, spatial_transform, get_loader, data_type, audio_transform)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,                 \n\u001b[1;32m     55\u001b[0m              annotation_path, \u001b[38;5;66;03m#annotation file\u001b[39;00m\n\u001b[1;32m     56\u001b[0m              subset, \u001b[38;5;66;03m#train test validation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m              data_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbiovisual\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     60\u001b[0m              audio_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m#adding noise\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/nfsmount/majid/multimodal/Emotion_recognition_T/datasets/ravdess.py:42\u001b[0m, in \u001b[0;36mmake_dataset\u001b[0;34m(subset, annotation_path)\u001b[0m\n\u001b[1;32m     40\u001b[0m dataset \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m annots:\n\u001b[0;32m---> 42\u001b[0m     index, video, start_frame, end_frame, biometric, start_row, end_row, stress \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)      \u001b[38;5;66;03m#import information from annotation file  \u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainvaltest\u001b[38;5;241m.\u001b[39mrstrip() \u001b[38;5;241m!=\u001b[39m subset:\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 8)"
     ]
    }
   ],
   "source": [
    "for fold in range(n_folds):\n",
    "\n",
    "    print(opt)\n",
    "    with open(os.path.join(opt.result_path, 'opts'+str(time.time())+str(fold)+'.json'), 'w') as opt_file:\n",
    "        json.dump(vars(opt), opt_file)\n",
    "\n",
    "    torch.manual_seed(opt.manual_seed)\n",
    "\n",
    "    ################IMPORTANT THE MODEL AND PARAMETERS are put here IMPORTANT#################\n",
    "    model, parameters = generate_model(opt)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(opt.device)\n",
    "\n",
    "    if not opt.no_train:\n",
    "\n",
    "        #define training data\n",
    "        training_data = get_training_set(opt, spatial_transform=None) # we change the spatial transform to none to prevent generating random noise  \n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            training_data,\n",
    "            batch_size=opt.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=opt.n_threads,\n",
    "            pin_memory=True)\n",
    "\n",
    "        train_logger = Logger(\n",
    "            os.path.join(opt.result_path, 'train'+str(fold)+'.log'),\n",
    "            ['epoch', 'loss', 'prec1', 'prec5', 'lr'])\n",
    "        train_batch_logger = Logger(\n",
    "            os.path.join(opt.result_path, 'train_batch'+str(fold)+'.log'),\n",
    "            ['epoch', 'batch', 'iter', 'loss', 'prec1', 'prec5', 'lr'])\n",
    "\n",
    "\n",
    "        optimizer = optim.SGD(\n",
    "            parameters,\n",
    "            lr=opt.learning_rate,\n",
    "            momentum=opt.momentum,\n",
    "            dampening=opt.dampening,\n",
    "            weight_decay=opt.weight_decay,\n",
    "            nesterov=False)\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, 'min', patience=opt.lr_patience)\n",
    "\n",
    "    if not opt.no_val:\n",
    "        video_transform = transforms.Compose([\n",
    "            transforms.ToTensor(opt.video_norm_value)])     \n",
    "\n",
    "        validation_data = get_validation_set(opt, spatial_transform=video_transform)\n",
    "\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            validation_data,\n",
    "            batch_size=opt.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=opt.n_threads,\n",
    "            pin_memory=True)\n",
    "\n",
    "        val_logger = Logger(\n",
    "                os.path.join(opt.result_path, 'val'+str(fold)+'.log'), ['epoch', 'loss', 'prec1', 'prec5'])\n",
    "        test_logger = Logger(\n",
    "                os.path.join(opt.result_path, 'test'+str(fold)+'.log'), ['epoch', 'loss', 'prec1', 'prec5'])\n",
    "\n",
    "\n",
    "    best_prec1 = 0\n",
    "    best_loss = 1e10\n",
    "    if opt.resume_path:\n",
    "        print('loading checkpoint {}'.format(opt.resume_path))\n",
    "        checkpoint = torch.load(opt.resume_path)\n",
    "        assert opt.arch == checkpoint['arch']\n",
    "        best_prec1 = checkpoint['best_prec1']\n",
    "        opt.begin_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    for i in range(opt.begin_epoch, opt.n_epochs + 1):\n",
    "\n",
    "        if not opt.no_train:\n",
    "            adjust_learning_rate(optimizer, i, opt)\n",
    "            train_epoch(i, train_loader, model, criterion, optimizer, opt,\n",
    "                        train_logger, train_batch_logger)\n",
    "            state = {\n",
    "                'epoch': i,\n",
    "                'arch': opt.arch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'best_prec1': best_prec1\n",
    "                }\n",
    "            save_checkpoint(state, False, opt, fold)\n",
    "\n",
    "        if not opt.no_val:\n",
    "\n",
    "            validation_loss, prec1 = val_epoch(i, val_loader, model, criterion, opt,\n",
    "                                        val_logger)\n",
    "            is_best = prec1 > best_prec1\n",
    "            best_prec1 = max(prec1, best_prec1)\n",
    "            state = {\n",
    "            'epoch': i,\n",
    "            'arch': opt.arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'best_prec1': best_prec1\n",
    "            }\n",
    "\n",
    "            save_checkpoint(state, is_best, opt, fold)\n",
    "\n",
    "\n",
    "    if opt.test:\n",
    "\n",
    "        test_logger = Logger(\n",
    "                os.path.join(opt.result_path, 'test'+str(fold)+'.log'), ['epoch', 'loss', 'prec1', 'prec5'])\n",
    "\n",
    "        video_transform = transforms.Compose([\n",
    "            transforms.ToTensor(opt.video_norm_value)])\n",
    "\n",
    "        test_data = get_test_set(opt, spatial_transform=video_transform) \n",
    "\n",
    "        #load best model\n",
    "        best_state = torch.load('%s/%s_best' % (opt.result_path, opt.store_name)+str(fold)+'.pth')\n",
    "        model.load_state_dict(best_state['state_dict'])\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            test_data,\n",
    "            batch_size=opt.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=opt.n_threads,\n",
    "            pin_memory=True)\n",
    "\n",
    "        test_loss, test_prec1 = val_epoch(10000, test_loader, model, criterion, opt,\n",
    "                                        test_logger)\n",
    "\n",
    "        with open(os.path.join(opt.result_path, 'test_set_bestval'+str(fold)+'.txt'), 'a') as f:\n",
    "                f.write('Prec1: ' + str(test_prec1) + '; Loss: ' + str(test_loss))\n",
    "        test_accuracies.append(test_prec1) \n",
    "\n",
    "\n",
    "with open(os.path.join(opt.result_path, 'test_set_bestval.txt'), 'a') as f:\n",
    "    f.write('Prec1: ' + str(np.mean(np.array(test_accuracies))) +'+'+str(np.std(np.array(test_accuracies))) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8fe89-fd32-421b-8548-f0a24cb9aab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
